#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KR Market - Historical Institutional Data Collector
ê³¼ê±° 60ì¼ ì™¸ì¸/ê¸°ê´€ ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ê¸°

ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ì¼ë³„ ì™¸ì¸/ê¸°ê´€ ìˆœë§¤ë§¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬
ë‚ ì§œë³„ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì¥ê¸° ë°±í…ŒìŠ¤íŠ¸ê°€ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.

ì¶œë ¥ íŒŒì¼: kr_market/historical_institutional_data.csv
ì»¬ëŸ¼: ticker, date, foreign_net, inst_net, volume, close_price
"""
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
import os
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class HistoricalInstitutionalCollector:
    """ê³¼ê±° ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, data_dir: str = None):
        self.data_dir = data_dir or os.path.dirname(os.path.abspath(__file__))
        self.output_path = os.path.join(self.data_dir, 'historical_institutional_data.csv')
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept-Language': 'ko-KR,ko;q=0.9',
        })
        
        self.request_delay = 0.2  # ìš”ì²­ ê°„ê²©
        
        logger.info("âœ… Historical Institutional Data Collector ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_stock_list(self, limit: int = None) -> list:
        """ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
        csv_path = os.path.join(os.path.dirname(self.data_dir), 'korean_stocks_list.csv')
        
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path)
            df['ticker'] = df['ticker'].astype(str).str.zfill(6)
            tickers = df['ticker'].tolist()
            
            if limit:
                tickers = tickers[:limit]
            
            logger.info(f"ğŸ“Š {len(tickers)}ê°œ ì¢…ëª© ë¡œë“œë¨")
            return tickers
        else:
            logger.error(f"âŒ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì—†ìŒ: {csv_path}")
            return []
    
    def scrape_daily_data(self, ticker: str) -> list:
        """
        ë‹¨ì¼ ì¢…ëª©ì˜ ì¼ë³„ ìˆ˜ê¸‰ ë°ì´í„° ìŠ¤í¬ë˜í•‘ (ìµœëŒ€ 60ì¼)
        
        Returns:
            List of dicts with keys: ticker, date, foreign_net, inst_net, volume, close_price
        """
        try:
            url = f"https://finance.naver.com/item/frgn.naver?code={ticker}"
            response = self.session.get(url, timeout=10)
            response.encoding = 'euc-kr'
            
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            daily_data = []
            tables = soup.find_all('table', class_='type2')
            
            for table in tables:
                rows = table.find_all('tr')
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    
                    if len(cells) >= 7:
                        try:
                            # ë‚ ì§œ í™•ì¸
                            date_cell = cells[0].get_text(strip=True)
                            if not re.match(r'\d{4}\.\d{2}\.\d{2}', date_cell):
                                continue
                            
                            # ë°ì´í„° íŒŒì‹±
                            close_price = self._parse_number(cells[1].get_text(strip=True))
                            volume = self._parse_number(cells[4].get_text(strip=True))
                            inst_net = self._parse_signed_number(cells[5].get_text(strip=True))
                            foreign_net = self._parse_signed_number(cells[6].get_text(strip=True))
                            
                            if volume > 0:
                                # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (2024.12.28 -> 2024-12-28)
                                date_formatted = date_cell.replace('.', '-')
                                
                                daily_data.append({
                                    'ticker': ticker,
                                    'date': date_formatted,
                                    'foreign_net': foreign_net,
                                    'inst_net': inst_net,
                                    'volume': volume,
                                    'close_price': close_price
                                })
                            
                            if len(daily_data) >= 60:
                                break
                                
                        except (IndexError, ValueError):
                            continue
                
                if len(daily_data) >= 60:
                    break
            
            time.sleep(self.request_delay)
            return daily_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ {ticker} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_number(self, text: str) -> int:
        """ìˆ«ì íŒŒì‹±"""
        text = re.sub(r'[,\s]', '', text)
        numbers = re.findall(r'\d+', text)
        return int(numbers[0]) if numbers else 0
    
    def _parse_signed_number(self, text: str) -> int:
        """ë¶€í˜¸ í¬í•¨ ìˆ«ì íŒŒì‹±"""
        text = re.sub(r'[,\s]', '', text)
        
        if '+' in text or 'â–²' in text:
            numbers = re.findall(r'\d+', text)
            return int(numbers[0]) if numbers else 0
        elif '-' in text or 'â–¼' in text:
            numbers = re.findall(r'\d+', text)
            return -int(numbers[0]) if numbers else 0
        else:
            numbers = re.findall(r'\d+', text)
            return int(numbers[0]) if numbers else 0
    
    def collect_all(self, max_stocks: int = None, max_workers: int = 10) -> pd.DataFrame:
        """
        ì „ì²´ ì¢…ëª© ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            max_stocks: ìµœëŒ€ ì¢…ëª© ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
            max_workers: ë™ì‹œ ìŠ¤ë ˆë“œ ìˆ˜
        """
        logger.info("ğŸš€ ê³¼ê±° ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        tickers = self.load_stock_list(limit=max_stocks)
        
        if not tickers:
            return pd.DataFrame()
        
        all_data = []
        success_count = 0
        fail_count = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self.scrape_daily_data, t): t 
                      for t in tickers}
            
            for future in tqdm(as_completed(futures), total=len(futures), 
                              desc="ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘"):
                ticker = futures[future]
                try:
                    data = future.result()
                    if data:
                        all_data.extend(data)
                        success_count += 1
                    else:
                        fail_count += 1
                except Exception as e:
                    logger.warning(f"âš ï¸ {ticker} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    fail_count += 1
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(all_data)
        
        if not df.empty:
            # ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
            df = df.sort_values(['ticker', 'date'])
            df = df.drop_duplicates(subset=['ticker', 'date'])
            
            # ì €ì¥
            df.to_csv(self.output_path, index=False, encoding='utf-8-sig')
            
            logger.info(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ!")
            logger.info(f"   ì„±ê³µ: {success_count}ê°œ ì¢…ëª©")
            logger.info(f"   ì‹¤íŒ¨: {fail_count}ê°œ ì¢…ëª©")
            logger.info(f"   ì´ ë ˆì½”ë“œ: {len(df):,}ê°œ")
            logger.info(f"   ê¸°ê°„: {df['date'].min()} ~ {df['date'].max()}")
            logger.info(f"   ì €ì¥: {self.output_path}")
        
        return df
    
    def generate_signals_from_history(self, lookback_days: int = 5) -> pd.DataFrame:
        """
        íˆìŠ¤í† ë¦¬ ë°ì´í„°ë¡œë¶€í„° ì¼ë³„ ì‹œê·¸ë„ ìƒì„±
        
        ê° ë‚ ì§œë³„ë¡œ "ê·¸ ë‚  ê¸°ì¤€ ê³¼ê±° Nì¼" ìˆ˜ê¸‰ì„ ê³„ì‚°í•˜ì—¬
        ì‹œê·¸ë„(ìŒëŒì´ ë“±)ì„ íŒë‹¨í•©ë‹ˆë‹¤.
        """
        if not os.path.exists(self.output_path):
            logger.error("âŒ íˆìŠ¤í† ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € collect_all()ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return pd.DataFrame()
        
        logger.info("ğŸ“Š íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì‹œê·¸ë„ ìƒì„± ì¤‘...")
        
        df = pd.read_csv(self.output_path)
        df['date'] = pd.to_datetime(df['date'])
        
        # ë‚ ì§œë³„ ì‹œê·¸ë„ ê³„ì‚°
        signals = []
        unique_dates = df['date'].unique()
        unique_dates = sorted(unique_dates)[lookback_days:]  # lookback ì´í›„ ë‚ ì§œë§Œ
        
        for signal_date in tqdm(unique_dates, desc="ì‹œê·¸ë„ ìƒì„±"):
            # í•´ë‹¹ ë‚ ì§œ ê¸°ì¤€ ê³¼ê±° Nì¼ ë°ì´í„°
            start_date = signal_date - pd.Timedelta(days=lookback_days)
            
            for ticker in df['ticker'].unique():
                ticker_data = df[(df['ticker'] == ticker) & 
                                (df['date'] > start_date) & 
                                (df['date'] <= signal_date)]
                
                if len(ticker_data) >= lookback_days - 1:
                    foreign_sum = ticker_data['foreign_net'].sum()
                    inst_sum = ticker_data['inst_net'].sum()
                    
                    # ì—°ì† ë§¤ìˆ˜ì¼ ê³„ì‚°
                    foreign_consecutive = 0
                    for _, row in ticker_data.sort_values('date', ascending=False).iterrows():
                        if row['foreign_net'] > 0:
                            foreign_consecutive += 1
                        else:
                            break
                    
                    # ìŒëŒì´ íŒë‹¨
                    is_double_buy = (foreign_sum > 0 and inst_sum > 0 and 
                                    foreign_consecutive >= 3)
                    
                    if is_double_buy:
                        signals.append({
                            'ticker': ticker,
                            'signal_date': signal_date,
                            'foreign_sum': foreign_sum,
                            'inst_sum': inst_sum,
                            'consecutive_days': foreign_consecutive,
                            'close_price': ticker_data.iloc[-1]['close_price']
                        })
        
        signals_df = pd.DataFrame(signals)
        
        if not signals_df.empty:
            signals_path = os.path.join(self.data_dir, 'historical_signals.csv')
            signals_df.to_csv(signals_path, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… ì‹œê·¸ë„ ì €ì¥: {signals_path}")
            logger.info(f"   ì´ ì‹œê·¸ë„: {len(signals_df)}ê°œ")
        
        return signals_df


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    collector = HistoricalInstitutionalCollector(
        data_dir=os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')
    )
    
    # ì „ì²´ ìˆ˜ì§‘ (í…ŒìŠ¤íŠ¸: 100ê°œ ì¢…ëª©ë§Œ)
    # ì „ì²´ 2760ê°œ ì¢…ëª© ìˆ˜ì§‘ ì‹œ max_stocks=None
    df = collector.collect_all(max_stocks=100, max_workers=10)
    
    if not df.empty:
        print("\n" + "=" * 60)
        print("ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ")
        print("=" * 60)
        print(df.head(20).to_string())
        
        # ì‹œê·¸ë„ ìƒì„±
        print("\nğŸ“Š ì‹œê·¸ë„ ìƒì„± ì¤‘...")
        signals = collector.generate_signals_from_history(lookback_days=5)
        
        if not signals.empty:
            print("\n" + "=" * 60)
            print("ğŸ¯ ìƒì„±ëœ ì‹œê·¸ë„ ìƒ˜í”Œ")
            print("=" * 60)
            print(signals.head(20).to_string())


if __name__ == "__main__":
    main()
